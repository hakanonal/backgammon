{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backgomman Data Structure And Agent Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition\n",
    "\n",
    "There are 4 sets of 6 locations, and +2 BAR locations for each player.\n",
    "\n",
    "Each of these locations can take between valuse between -15 and +15, Where + means one player and - means other player's cheker 0 means the location is empty.\n",
    "\n",
    "0th location is the - players BAR and 25th locations is the + players BAR.\n",
    "\n",
    "the 26th and 27th indexed integers are the rolled dice to be played. The last two number represents the dice outcome each can take values between 1 and 6.\n",
    "\n",
    "General state outcome is ((15+15+1)^26)*(6^2)\n",
    "\n",
    "This data structure is inspired from: https://scholarworks.rit.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=7617&context=theses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_state = [0,  -2, 0, 0, 0, 0,+5,   0,+3, 0, 0, 0,-5,   +5, 0, 0, 0, -3, 0,  -5, 0, 0, 0, 0,+2,  0,   4,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Definition\n",
    "\n",
    "Action space is a touple each represents the dice, where each element is between 0 and 25 (the index of the location where a checker to advance as the dice number)\n",
    "\n",
    "The following example is the perfect action for the initial state to take a gate at your closest home area if the player is + and -.\n",
    "\n",
    "Action Space is: 26^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: if the dice is same the action must not be two element but 4 element tuple\n",
    "actionw = (6,8)      #best known action for + player\n",
    "actionb = (19,17)    #best known action for - player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from os import path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Conv2DTranspose, BatchNormalization, UpSampling2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.activations import softmax\n",
    "import wandb\n",
    "\n",
    "class agent:\n",
    "\n",
    "    def __init__(self,color, discount=0.95,exploration_rate=0.9,decay_factor=0.9999):\n",
    "        self.color = color # value must be -1 or +1 (-1 for black and +1 is for white)\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = exploration_rate # Initial exploration rate\n",
    "        self.decay_factor = decay_factor\n",
    "\n",
    "        #wandb.config.update({'model_name':'Dense,208-104-52 with relu, adam, mean_squared_error'})\n",
    "        if(path.exists(self._getModelFilename())):\n",
    "            self.model = load_model(self._getModelFilename())\n",
    "        else:\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Dense(208,activation=\"relu\", input_shape=(28,)))\n",
    "            self.model.add(Dense(104,activation=\"relu\"))\n",
    "            self.model.add(Dense(52))\n",
    "            self.model.add(Reshape((2,26)))\n",
    "            self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def _getModelFilename(self):\n",
    "        return \"agent_model%d.h5\" % self.color\n",
    "    \n",
    "    def _getBARi(self):\n",
    "        if(self.color == 1):\n",
    "            return 25\n",
    "        return 0\n",
    "    def _getLOpponentBARi(self):\n",
    "        if(self.color == 1):\n",
    "            return 0\n",
    "        return 25\n",
    "\n",
    "\n",
    "    def _getHomeRange(self):\n",
    "        if(self.color == 1): \n",
    "            return range(1,7)\n",
    "        return range(19,25)\n",
    "\n",
    "    def _getNotHomeRange(self):\n",
    "        if(self.color == 1): \n",
    "            return range(7,26)\n",
    "        return range(0,19)\n",
    "\n",
    "\n",
    "    def _getTotalCheckersAtHome(self,state):\n",
    "        t1 = 0\n",
    "        for i in self._getHomeRange():\n",
    "            if((state[i] * self.color) > 0):\n",
    "                t1 += (state[i] * self.color)\n",
    "        return t1\n",
    "    def _getTotalCheckersAtNotHome(self,state):\n",
    "        t1 = 0\n",
    "        for i in self._getNotHomeRange():\n",
    "            if((state[i] * self.color) > 0):\n",
    "                t1 += (state[i] * self.color)\n",
    "        return t1\n",
    "\n",
    "\n",
    "    def play(self,old_state,action):\n",
    "        di1,di2 = action\n",
    "        dice1 = old_state[26] \n",
    "        dice2 = old_state[27]\n",
    "\n",
    "        reward = 0\n",
    "        new_state,r = self.playSingle(old_state,dice1,di1)\n",
    "        reward += r\n",
    "        new_state,r = self.playSingle(new_state,dice2,di2)\n",
    "        reward += r\n",
    "        if(dice1 == dice2):\n",
    "            di3,di4 = self.get_next_action(new_state)\n",
    "            new_state,r = self.playSingle(new_state,dice1,di3)\n",
    "            reward += r\n",
    "            new_state,r = self.playSingle(new_state,dice2,di4)\n",
    "            reward += r\n",
    "            \n",
    "        return new_state,reward\n",
    "\n",
    "\n",
    "    def playSingle(self,old_state,dice,action):\n",
    "        state = old_state.copy()\n",
    "        dice = dice * self.color\n",
    "        bari = self._getBARi()\n",
    "\n",
    "        #These are invalid plays and return negative points.\n",
    "        #can not play other players checker or empty location\n",
    "        if(state[action] * self.color <= 0):\n",
    "            return state,-20        \n",
    "        #can not play outside from 26 locations (including bars).\n",
    "        if(action not in range(26)):\n",
    "            return state,-20\n",
    "        #can not play outside to 24 locations. TODO: this is not valid when collecting checkers at the end of the game.\n",
    "        if(action-dice not in range(1,25)):\n",
    "            return state,-20\n",
    "        #can not play from other than BAR if there is a checker in BAR\n",
    "        if(state[bari] != 0 and action != bari):\n",
    "            return state,-20\n",
    "        #if BAR has checker and can not play to destination it is not invalid. do not punish\n",
    "        if(state[bari] != 0 and (state[action-dice] * self.color < -1)):\n",
    "            return state, 0\n",
    "        #can not play to location where opponent's has more than 1 checkers.\n",
    "        if(state[action-dice] * self.color < -1):\n",
    "            return state,-20        \n",
    "\n",
    "        #advance state\n",
    "        reward = 20\n",
    "        state[action] -= self.color\n",
    "        if(state[action-dice] * self.color == -1):\n",
    "            state[action-dice] = 0\n",
    "            state[self._getLOpponentBARi()] -= self.color\n",
    "            reward += 5\n",
    "        state[action-dice] += self.color\n",
    "        #TODO: check if checkers must be collected or not according to if checkers are at the home or not.\n",
    "       \n",
    "\n",
    "        #punish if target or source location has only one checker\n",
    "        if(abs(state[action-dice]) == 1):\n",
    "            reward -= 3\n",
    "        if(abs(state[action]) == 1):\n",
    "            reward -= 3\n",
    "\n",
    "\n",
    "        #declaring win when all chekers at home.TODO: check and give max point all checkers are collected. win game.\n",
    "        t1 = self._getTotalCheckersAtHome(state)\n",
    "        if(t1 == 15):\n",
    "            reward = 50\n",
    "\n",
    "        return state,reward\n",
    "        \n",
    "\n",
    "    def playAll(self,state):\n",
    "        y = np.empty((26,26))        \n",
    "        for i in range(26):\n",
    "            for j in range(26):\n",
    "                _,reward = self.play(state,(i,j))\n",
    "                y[i][j] = reward\n",
    "        return y\n",
    "\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        if random.random() > self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.greedy_action(state)\n",
    "        else:\n",
    "            return self.random_action()\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        return tuple(np.argmax(self.getQ(state),axis=1))\n",
    "    def random_action(self):\n",
    "        return (random.randint(0,25),random.randint(0,25))\n",
    "    def best_action(self,state):\n",
    "        imax = np.argmax(self.playAll(state))        \n",
    "        return ( int((imax-(imax%26))/26) ,int(imax%26))\n",
    "\n",
    "    def getQ(self,state):\n",
    "        state_to_predict = np.expand_dims(state,0)\n",
    "        action_prediction = self.model.predict(state_to_predict)\n",
    "        return action_prediction[0]\n",
    "\n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        \n",
    "        old_state_prediction = self.getQ(old_state)\n",
    "        new_state_prediction = self.getQ(new_state)\n",
    "        a1,a2 = action\n",
    "\n",
    "        old_state_prediction[0][a1] = reward + self.discount * np.amax(new_state_prediction[0])\n",
    "        old_state_prediction[1][a2] = reward + self.discount * np.amax(new_state_prediction[1])\n",
    "\n",
    "        x = np.expand_dims(old_state,0)\n",
    "        y = np.expand_dims(old_state_prediction,0)\n",
    "        self.model.fit(x,y,verbose=0)\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):        \n",
    "        self.train(old_state, new_state, action, reward)\n",
    "        self.exploration_rate *= self.decay_factor\n",
    "        #self.saveModel()\n",
    "\n",
    "    def saveModel(self):\n",
    "        #self.model.save(self._getModelFilename())\n",
    "        #wandb.save(self._getModelFilename())\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Board Definition Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "class environment:\n",
    "\n",
    "    def __init__(self,args=''):\n",
    "        self.args = self._parseArgs(args)\n",
    "        #wandb.init(project=\"tavla2\",name=self.args.run_name)\n",
    "        #wandb.config.update(self.args)\n",
    "        self.white_agent = agent(color=1,discount=self.args.discount,exploration_rate=self.args.exploration_rate,decay_factor=self.args.decay_factor)\n",
    "        self.black_agent = agent(color=-1,discount=self.args.discount,exploration_rate=self.args.exploration_rate,decay_factor=self.args.decay_factor)\n",
    "        self.white_max_reward = 0\n",
    "        self.black_max_reward = 0\n",
    "        self.white_tot_reward = 0\n",
    "        self.black_tot_reward = 0\n",
    "        self.r_avg_list = [[],[]]\n",
    "        self.total_penalty = 0\n",
    "        self.total_valid = 0        \n",
    "        self._initGame()\n",
    "\n",
    "\n",
    "    def _initGame(self):\n",
    "        self.white_reward = 0\n",
    "        self.black_reward = 0\n",
    "        self.state = [0,  -2, 0, 0, 0, 0,+5,   0,+3, 0, 0, 0,-5,   +5, 0, 0, 0, -3, 0,  -5, 0, 0, 0, 0,+2,  0,   4,6]  #initial sate\n",
    "        #self.state = [0,   4, 0, 6, 0, 0, 0,   1, 1, 0, 0, 0, 0,    0, 0, 0, 1, -1, 1,   1, 0, 0, 0, 0,-14, 0,   2,3]  #test state\n",
    "        self.turn = random.randrange(-1,2,2) #The first round is decided number (-1 or 1)\n",
    "    \n",
    "    def roll(self):\n",
    "        self.state[26] = random.randint(1,6)\n",
    "        self.state[27] = random.randint(1,6)\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        for game_no in range(1,self.args.episode+1):\n",
    "            reward = 0\n",
    "            self._initGame()\n",
    "            self.roll()\n",
    "            i = 1\n",
    "            while reward != 50:\n",
    "                clear_output(wait=True)\n",
    "                self.render()\n",
    "                #self._plot()\n",
    "                if(self.turn == 1):\n",
    "                    if( np.amax(self.white_agent.playAll(self.state)) < 0 ):\n",
    "                        break\n",
    "                    action_to_play = self.white_agent.get_next_action(self.state)\n",
    "                    new_state, reward = self.white_agent.play(self.state, action_to_play)\n",
    "                    self.white_agent.update(old_state=self.state,new_state=new_state,action=action_to_play,reward=reward)\n",
    "                    self.state = new_state\n",
    "                    self.white_reward += reward\n",
    "                    self.white_tot_reward += reward\n",
    "                    self.white_max_reward = max(self.white_max_reward,self.white_reward)\n",
    "                if(self.turn == -1):\n",
    "                    if( np.amax(self.black_agent.playAll(self.state)) < 0 ):\n",
    "                        break\n",
    "                    action_to_play = self.black_agent.get_next_action(self.state)\n",
    "                    new_state, reward = self.black_agent.play(self.state, action_to_play)\n",
    "                    self.black_agent.update(old_state=self.state,new_state=new_state,action=action_to_play,reward=reward)\n",
    "                    self.state = new_state\n",
    "                    self.black_reward += reward\n",
    "                    self.black_tot_reward += reward\n",
    "                    self.black_max_reward = max(self.black_max_reward,self.black_reward)\n",
    "                print(action_to_play)\n",
    "                self.render()            \n",
    "                print(reward)\n",
    "                print(self.state)\n",
    "                if(reward >= 0):\n",
    "                    self.turn *= -1\n",
    "                    self.roll()\n",
    "                    i = 1\n",
    "                    self.total_valid += 1\n",
    "                    input(\"continue...\")\n",
    "                else:\n",
    "                    i += 1\n",
    "                    self.total_penalty += 1\n",
    "                    #break\n",
    "                    \n",
    "\n",
    "            metrics = {\n",
    "                'full-game' : (game_no-self.total_penalty),\n",
    "                'full-game-rate' : ((game_no-self.total_penalty)/game_no),\n",
    "                'valid-total' : self.total_valid,\n",
    "                'valid-avarage' : self.total_valid/game_no,\n",
    "                'exploration-rate-white' : self.white_agent.exploration_rate,\n",
    "                'exploration-rate-black' : self.black_agent.exploration_rate,\n",
    "                'max-reward-white' : self.white_max_reward,\n",
    "                'max-reward-black' : self.black_max_reward,\n",
    "                'tot-reward-white' : self.white_tot_reward,\n",
    "                'tot-reward-black' : self.black_tot_reward,\n",
    "                'avg-reward-white' : self.white_tot_reward/game_no,\n",
    "                'avg-reward-black' : self.black_tot_reward/game_no,\n",
    "                'reward-white':self.white_reward,\n",
    "                'reward-black':self.black_reward\n",
    "            }\n",
    "            if False:#game_no % 1000 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(\"Game           : %d\"%game_no)\n",
    "                print(\"Full Game      : %d - %f \"%(metrics['full-game'],metrics['full-game-rate']))\n",
    "                print(\"Valid          : %d - %f \"%(metrics['valid-total'],metrics['valid-avarage']))\n",
    "                print(\"Exp Rate       : \\x1b[6;30;47m%f\\x1b[0m \\x1b[0;37;40m%f\\x1b[0m\" % (metrics['exploration-rate-white'],metrics['exploration-rate-black']))\n",
    "                print(\"Max Rewards    : \\x1b[6;30;47m%f\\x1b[0m \\x1b[0;37;40m%f\\x1b[0m\" % (metrics['max-reward-white'],metrics['max-reward-black']))\n",
    "                print(\"Tot Rewards    : \\x1b[6;30;47m%f\\x1b[0m \\x1b[0;37;40m%f\\x1b[0m\" % (metrics['tot-reward-white'],metrics['tot-reward-black']))\n",
    "                print(\"Avg Rewards    : \\x1b[6;30;47m%f\\x1b[0m \\x1b[0;37;40m%f\\x1b[0m\" % (metrics['avg-reward-white'],metrics['avg-reward-black']))\n",
    "            #wandb.log(metrics,step=game_no)\n",
    "            self.r_avg_list[0].append(self.white_reward)\n",
    "            self.r_avg_list[1].append(self.black_reward)\n",
    "\n",
    "        self._plotAvgReward()\n",
    "        self.white_agent.saveModel()\n",
    "        self.black_agent.saveModel()\n",
    "            \n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "    def _getChecker(self,number):\n",
    "        _sign = lambda x: x and (1, -1)[x<0]\n",
    "        if(_sign(number) == 1):\n",
    "            return(\"\\x1b[6;30;47m%d\\x1b[0m\" % number)\n",
    "        elif(_sign(number) == -1):\n",
    "            return(\"\\x1b[0;37;40m%d\\x1b[0m\" % abs(number))\n",
    "        return \"+\"\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        print(\"Dice: [%d] [%d]\" % (self.state[26],self.state[27]))\n",
    "        print(\"Reward: \\x1b[6;30;47m%d\\x1b[0m \\x1b[0;37;40m%d\\x1b[0m\" % (self.white_reward,self.black_reward))\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"++432109++876543++\")\n",
    "        print(\"++------++------++\")\n",
    "        _line_to_print  = self._getChecker(self.state[25])\n",
    "        _line_to_print += \"+\"\n",
    "        for i in reversed(range(19,25)): ##########\n",
    "            if(self.state[i] != 0):\n",
    "                _line_to_print += self._getChecker(self.state[i])\n",
    "            else:\n",
    "                _line_to_print += \" \"\n",
    "        _line_to_print += \"++\"\n",
    "\n",
    "        for i in reversed(range(13,19)): ##########\n",
    "            if(self.state[i] != 0):\n",
    "                _line_to_print += self._getChecker(self.state[i])\n",
    "            else:\n",
    "                _line_to_print += \" \"\n",
    "        _line_to_print += \"++\"\n",
    "        print(_line_to_print)\n",
    "        print(\"++      ++      ++\")\n",
    "\n",
    "        _line_to_print  = self._getChecker(self.state[0])\n",
    "        _line_to_print += \"+\"\n",
    "        for i in range(1,7): ##########\n",
    "            if(self.state[i] != 0):\n",
    "                _line_to_print += self._getChecker(self.state[i])\n",
    "            else:\n",
    "                _line_to_print += \" \"\n",
    "        _line_to_print += \"++\"\n",
    "\n",
    "        for i in range(7,13): ##########\n",
    "            if(self.state[i] != 0):\n",
    "                _line_to_print += self._getChecker(self.state[i])\n",
    "            else:\n",
    "                _line_to_print += \" \"\n",
    "        _line_to_print += \"++\"\n",
    "        print(_line_to_print)\n",
    "        print(\"++------++------++\")\n",
    "        print(\"++123456++789012++\")\n",
    "\n",
    "    def _plot(self):\n",
    "        fig, ax = plt.subplots()\n",
    "        if(self.turn == 1):\n",
    "            data = self.white_agent.getQ(self.state)\n",
    "            fig.suptitle(\"White\")\n",
    "        if(self.turn == -1):\n",
    "            data = self.black_agent.getQ(self.state)\n",
    "            fig.suptitle(\"Black\")\n",
    "        for line in data:\n",
    "            keys = range(len(line))\n",
    "            ax.plot(keys,line)\n",
    "        plt.show()\n",
    "    \n",
    "    def _plotAvgReward(self):\n",
    "        fig, ax = plt.subplots()\n",
    "        for line in self.r_avg_list:\n",
    "            keys = range(len(line))\n",
    "            ax.plot(keys,line)\n",
    "        plt.show()\n",
    "\n",
    "    def _parseArgs(self,args=''):\n",
    "        parser = argparse.ArgumentParser(description='Backgammon trainer program')\n",
    "        parser.add_argument(\n",
    "            '-d','--discount',\n",
    "            type=float,default=0.95,metavar='D',\n",
    "            help='Discount ratio for feature considiration (Default:0.95)'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '-er','--exploration_rate',\n",
    "            type=float,default=0.9,metavar='ER',\n",
    "            help='Initial exploration rate (Default:0.9) 1 is all actions random(always explore), 0 is no random (always exploit)'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '-df','--decay_factor',\n",
    "            type=float,default=0.9999,metavar='DF',\n",
    "            help='This number is multiplied with exploration rate in each action (Default:0.9999) If smaller that 1 exploration rate will decrease and every step system will use more exploit then explore.'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '-n','--run_name',\n",
    "            type=str,metavar='N',\n",
    "            help='Give an explenatory run name'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '-e','--episode',\n",
    "            type=int,default=100000,metavar='E',\n",
    "            help='Number of episodes (games) to try'\n",
    "        )\n",
    "        return parser.parse_args(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dice: [6] [2]\nReward: \u001b[6;30;47m333\u001b[0m \u001b[0;37;40m293\u001b[0m\n\n++432109++876543++\n++------++------++\n++\u001b[6;30;47m2\u001b[0m\u001b[0;37;40m2\u001b[0m\u001b[0;37;40m2\u001b[0m\u001b[0;37;40m4\u001b[0m\u001b[0;37;40m1\u001b[0m ++\u001b[0;37;40m3\u001b[0m\u001b[0;37;40m1\u001b[0m   \u001b[6;30;47m1\u001b[0m++\n++      ++      ++\n++\u001b[0;37;40m2\u001b[0m\u001b[6;30;47m3\u001b[0m\u001b[6;30;47m3\u001b[0m\u001b[6;30;47m3\u001b[0m\u001b[6;30;47m2\u001b[0m ++ \u001b[6;30;47m1\u001b[0m    ++\n++------++------++\n++123456++789012++\n(8, 4)\nDice: [6] [2]\nReward: \u001b[6;30;47m373\u001b[0m \u001b[0;37;40m293\u001b[0m\n\n++432109++876543++\n++------++------++\n++\u001b[6;30;47m2\u001b[0m\u001b[0;37;40m2\u001b[0m\u001b[0;37;40m2\u001b[0m\u001b[0;37;40m4\u001b[0m\u001b[0;37;40m1\u001b[0m ++\u001b[0;37;40m3\u001b[0m\u001b[0;37;40m1\u001b[0m   \u001b[6;30;47m1\u001b[0m++\n++      ++      ++\n++\u001b[0;37;40m2\u001b[0m\u001b[6;30;47m5\u001b[0m\u001b[6;30;47m3\u001b[0m\u001b[6;30;47m2\u001b[0m\u001b[6;30;47m2\u001b[0m ++      ++\n++------++------++\n++123456++789012++\n40\n[0, -2, 5, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, -3, 0, -1, -4, -2, -2, 2, 0, 6, 2]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c92483dee99c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d 0.4 -er 0 -df 1'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8623c498002e>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_valid\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"continue...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "board = environment('-d 0.4 -er 0 -df 1'.split())\n",
    "\n",
    "board.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = []\n",
    "x = range(10000)\n",
    "for i in x:\n",
    "    y.append((math.sin(i/500)/(1.5999*1000)*-1)+1)\n",
    "\n",
    "plt.plot(x, y, color = 'red', marker = \"o\")  \n",
    "plt.title(\"math.sin()\")  \n",
    "plt.xlabel(\"X\")  \n",
    "plt.ylabel(\"Y\")  \n",
    "plt.show()  \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "run = api.run(\"hakanonal/tavla2/zcj977l6\")\n",
    "run.file(\"agent_model1.h5\").download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bitb4053a4b20a84f3abb1bff9c403e377a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}